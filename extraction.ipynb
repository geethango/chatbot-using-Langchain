{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31m[IPKernelApp] ERROR | Failed to create history session in C:\\Users\\geeth\\.ipython\\profile_default\\history.sqlite. History will not be saved.\n",
      "\u001b[1;31mTraceback (most recent call last):\n",
      "\u001b[1;31m  File \"c:\\Users\\geeth\\OneDrive\\Desktop\\task2\\venv\\lib\\site-packages\\IPython\\core\\history.py\", line 551, in __init__\n",
      "\u001b[1;31m    self.new_session()\n",
      "\u001b[1;31m  File \"c:\\Users\\geeth\\OneDrive\\Desktop\\task2\\venv\\lib\\site-packages\\decorator.py\", line 235, in fun\n",
      "\u001b[1;31m    return caller(func, *(extras + args), **kw)\n",
      "\u001b[1;31m  File \"c:\\Users\\geeth\\OneDrive\\Desktop\\task2\\venv\\lib\\site-packages\\IPython\\core\\history.py\", line 61, in only_when_enabled\n",
      "\u001b[1;31m    return f(self, *a, **kw)\n",
      "\u001b[1;31m  File \"c:\\Users\\geeth\\OneDrive\\Desktop\\task2\\venv\\lib\\site-packages\\IPython\\core\\history.py\", line 583, in new_session\n",
      "\u001b[1;31m    cur = conn.execute(\n",
      "\u001b[1;31msqlite3.OperationalError: database or disk is full\n",
      "\u001b[1;31mAssertion failed: error not defined [8] (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpz6i45qz5\\build\\_deps\\bundled_libzmq-src\\src\\ip.cpp:464). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "#from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "#from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 links.\n",
      "Loading: mailto:support@brainlox.com\n",
      "Failed to load mailto:support@brainlox.com: No connection adapters were found for 'mailto:support@brainlox.com'\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# ✅ Step 1: Get all links from the webpage\n",
    "url = \"https://brainlox.com/courses/category/technical\"\n",
    "response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# ✅ Step 2: Extract all links from <a> tags\n",
    "links = []\n",
    "for a_tag in soup.find_all(\"a\", href=True):\n",
    "    link = a_tag[\"href\"]\n",
    "    if \"brainlox.com\" in link:  # Ensure it's an internal link\n",
    "        links.append(link)\n",
    "\n",
    "print(f\"Found {len(links)} links.\")\n",
    "\n",
    "# ✅ Step 3: Load data from each link using WebBaseLoader\n",
    "all_docs = []\n",
    "for link in links[:5]:  # Limiting to 5 links for testing\n",
    "    try:\n",
    "        print(f\"Loading: {link}\")\n",
    "        loader = WebBaseLoader(link)\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {link}: {e}\")\n",
    "\n",
    "# ✅ Step 4: Print extracted content (first 500 chars from first document)\n",
    "if all_docs:\n",
    "    print(\"\\nExtracted Content:\\n\", all_docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brainlox: Learn technical courses.Courses TechnicalAcademicLanguageMusicLifestyleBook a Free Demo NowSign InFAQContact UsHomeCoursesCoursesWe found great courses available for you$30per sessionLEARN SCRATCH PROGRAMING\n",
      "Scratch Course is the foundation of coding and is a building block of a coding journey. If you want 16 LessonsView Details$30per sessionLEARN CLOUD COMPUTING BASICS-AWS\n",
      "In this course we are going to cover the basics and the most important services on AWS,\n",
      "At the end  20 LessonsVie\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use a markdown file from github page\n",
    "loader = WebBaseLoader(\"https://brainlox.com/courses/category/technical\")\n",
    "\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "#Create a split of the document using the text splitter\n",
    "splits = text_splitter.split_documents(docs)\n",
    "# Extract text from Document objects\n",
    "splits_text = [doc.page_content for doc in splits]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "The paging file is too small for this operation to complete. (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\task2\\venv\\lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:59\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer(\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, cache_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\n\u001b[0;32m     61\u001b[0m )\n",
      "File \u001b[1;32md:\\task2\\venv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:308\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[0;32m    299\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[0;32m    302\u001b[0m     model_name_or_path,\n\u001b[0;32m    303\u001b[0m     token,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    307\u001b[0m ):\n\u001b[1;32m--> 308\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[0;32m    321\u001b[0m         model_name_or_path,\n\u001b[0;32m    322\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    330\u001b[0m     )\n",
      "File \u001b[1;32md:\\task2\\venv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1739\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[0;32m   1736\u001b[0m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class(model_name_or_path, cache_dir\u001b[38;5;241m=\u001b[39mcache_folder, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1740\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1741\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(model_name_or_path)\n",
      "File \u001b[1;32md:\\task2\\venv\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:81\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[0;32m     78\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     80\u001b[0m config, is_peft_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[0;32m     84\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[1;32md:\\task2\\venv\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:181\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[1;34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    182\u001b[0m         model_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_peft_model:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_peft_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_only_kwargs)\n",
      "File \u001b[1;32md:\\task2\\venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32md:\\task2\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32md:\\task2\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:4042\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4022\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m get_checkpoint_shard_files(\n\u001b[0;32m   4023\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   4024\u001b[0m         resolved_archive_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4034\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   4035\u001b[0m     )\n\u001b[0;32m   4037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4038\u001b[0m     is_safetensors_available()\n\u001b[0;32m   4039\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m   4040\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4041\u001b[0m ):\n\u001b[1;32m-> 4042\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   4043\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mmetadata()\n\u001b[0;32m   4045\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4046\u001b[0m         \u001b[38;5;66;03m# Assume it's a pytorch checkpoint (introduced for timm checkpoints)\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: The paging file is too small for this operation to complete. (os error 1455)"
     ]
    }
   ],
   "source": [
    "model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs/chroma/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ✅ Ensure you have an embedding model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#embedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# ✅ Store in ChromaDB\u001b[39;00m\n\u001b[0;32m     10\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[0;32m     11\u001b[0m     texts\u001b[38;5;241m=\u001b[39msplits_text,\n\u001b[1;32m---> 12\u001b[0m     embedding\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m,\n\u001b[0;32m     13\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# ✅ Print the number of stored vectors\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of stored embeddings:\u001b[39m\u001b[38;5;124m\"\u001b[39m, vectordb\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mcount())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "persist_directory = \"docs/chroma/\"\n",
    "\n",
    "# ✅ Ensure you have an embedding model\n",
    "#embedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ✅ Convert Documents to Text\n",
    "#texts = [doc.page_content for doc in splits]  # Extract text from Document objects\n",
    "\n",
    "# ✅ Store in ChromaDB\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=splits_text,\n",
    "    embedding=model,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# ✅ Print the number of stored vectors\n",
    "print(\"Number of stored embeddings:\", vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# ✅ Load a local model using Transformers\n",
    "#model_pipeline = pipeline(\"text-generation\", model=\"facebook/opt-1.3b\",max_new_tokens=200,)\n",
    "model_pipeline = pipeline(\"text-generation\", model=\"facebook/opt-350m\", max_new_tokens=200)\n",
    "\n",
    "\n",
    "# ✅ Define llm using HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=model_pipeline)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "This introduction to cloud computing on Amazon AWS course takes you from the AWS Ad 18 LessonsView Details$30per sessionPYTHON PROGRAMMING-BEGINNER\n",
      "Python is a language with simple syntax, and a powerful set of libraries. It has a rich programming 16 LessonsView Details$30per sessionRoblox Programming For BeginnersExplore the dynamic universe of game development with our \"Roblox Game Development Fundamentals\" cou 15 LessonsView Details$32per sessionPYTHON PROGRAMMING-INTERMEDIATE\n",
      "Take your python skills to the next level and start building real applications.\n",
      "Python is a pro 16 LessonsView Details$35per sessionPYTHON PROGRAMMING-ADVANCEIf you already know Python basics, then this training is the next step in your Python learning path  30 LessonsView Details$30per sessionPYTHON PROGRAMMING GROUP CLASSES - BEGINNER\n",
      "\n",
      "This introduction to cloud computing on Amazon AWS course takes you from the AWS Ad 18 LessonsView Details$30per sessionPYTHON PROGRAMMING-BEGINNER\n",
      "Python is a language with simple syntax, and a powerful set of libraries. It has a rich programming 16 LessonsView Details$30per sessionRoblox Programming For BeginnersExplore the dynamic universe of game development with our \"Roblox Game Development Fundamentals\" cou 15 LessonsView Details$32per sessionPYTHON PROGRAMMING-INTERMEDIATE\n",
      "Take your python skills to the next level and start building real applications.\n",
      "Python is a pro 16 LessonsView Details$35per sessionPYTHON PROGRAMMING-ADVANCEIf you already know Python basics, then this training is the next step in your Python learning path  30 LessonsView Details$30per sessionPYTHON PROGRAMMING GROUP CLASSES - BEGINNER\n",
      "\n",
      "Question: What are major courses in python\n",
      "Helpful Answer:\n",
      "\n",
      "Python is a language with simple syntax, and a powerful set of libraries. It has a rich programming 16 LessonsView Details$30per sessionRoblox Programming For BeginnersExplore the dynamic universe of game development with our \"Roblox Game Development Fundamentals\" cou 15 LessonsView Details$32per sessionPYTHON PROGRAMMING-INTERMEDIATE\n",
      "Take your python skills to the next level and start building real applications.\n",
      "Python is a pro 16 LessonsView Details$35per sessionPYTHON PROGRAMMING-ADVANCEIf you already know Python basics, then this training is the next step in your Python learning path  30 LessonsView Details$30per sessionPYTHON PROGRAMMING GROUP CLASSES - BEGINNER\n",
      "\n",
      "This introduction to cloud computing on Amazon AWS course takes you from the AWS Ad 18 LessonsView Details$30per sessionPYTHON PROGRAMMING-BEGINNER\n",
      "Python is a language with\n"
     ]
    }
   ],
   "source": [
    "# Pass question to the qa_chain\n",
    "question = \"What are major courses in python\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "#result[\"result\"]\n",
    "print(result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
